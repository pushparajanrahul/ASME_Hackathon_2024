{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c72e9a76-2b64-4272-a107-26b1b869e65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rpushpar/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/rpushpar/.local/lib/python3.9/site-packages/requests/__init__.py:109: RequestsDependencyWarning: urllib3 (2.2.2) or chardet (None)/charset_normalizer (3.3.2) doesn't match a supported version!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torchvision import transforms\n",
    "from timm import create_model\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "import requests\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torchvision.transforms import autoaugment, transforms\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c03c03dc-addb-4b9f-b2da-778523f5699e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required NLTK data\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Disable DecompressionBombWarning\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(filename='image_errors.log', level=logging.ERROR, \n",
    "                    format='%(asctime)s:%(levelname)s:%(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a27b78d-7947-4d1d-9424-b269e15076b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_normalize_label(label):\n",
    "    \"\"\"Clean and normalize a label.\"\"\"\n",
    "    label = str(label)\n",
    "    cleaned = ' '.join(label.lower().split())\n",
    "    cleaned = ''.join(e for e in cleaned if e.isalnum() or e.isspace())\n",
    "    return ' '.join(lemmatizer.lemmatize(word) for word in cleaned.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f8c7a9a-115e-44e0-a0b1-84fef30458e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactoryNetBBoxDataset(Dataset):\n",
    "    def __init__(self, coco_json, data_dir, transform=None):\n",
    "        with open(coco_json, 'r') as f:\n",
    "            self.coco_data = json.load(f)\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.image_info = {img['id']: img for img in self.coco_data['images']}\n",
    "        self.category_info = {cat['id']: cat for cat in self.coco_data['categories']}\n",
    "        \n",
    "        self.instances = []\n",
    "        for ann in self.coco_data['annotations']:\n",
    "            self.instances.append({\n",
    "                'image_id': ann['image_id'],\n",
    "                'bbox': ann['bbox'],\n",
    "                'category_id': ann['category_id']\n",
    "            })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.instances)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        instance = self.instances[idx]\n",
    "        img_info = self.image_info[instance['image_id']]\n",
    "        img_path = os.path.join(self.data_dir, img_info['file_name'])\n",
    "        \n",
    "        try:\n",
    "            with Image.open(img_path) as img:\n",
    "                # Crop the image to the bounding box\n",
    "                bbox = instance['bbox']\n",
    "                img = img.crop((bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]))\n",
    "                img = img.convert('RGB')\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error opening image {img_path}: {str(e)}\")\n",
    "            img = Image.new('RGB', (224, 224), color='gray')\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        label = instance['category_id']\n",
    "        \n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa7f31dd-3cf3-437a-a54a-3084b5b98080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_coco_dataset(data_dir, excluded_files):\n",
    "    coco_format = {\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": []\n",
    "    }\n",
    "    \n",
    "    category_id_map = {}\n",
    "    annotation_id = 1\n",
    "    image_id = 0\n",
    "    \n",
    "    all_files = [f for f in os.listdir(data_dir) if f.endswith('.jpg') and f not in excluded_files]\n",
    "    \n",
    "    for img_file in tqdm(all_files, desc=\"Creating COCO dataset\"):\n",
    "        csv_file = img_file.replace('.jpg', '.csv')\n",
    "        csv_path = os.path.join(data_dir, csv_file)\n",
    "        img_path = os.path.join(data_dir, img_file)\n",
    "        \n",
    "        if os.path.exists(csv_path):\n",
    "            try:\n",
    "                with Image.open(img_path) as img:\n",
    "                    width, height = img.size\n",
    "                \n",
    "                coco_format[\"images\"].append({\n",
    "                    \"id\": image_id,\n",
    "                    \"file_name\": img_file,\n",
    "                    \"width\": width,\n",
    "                    \"height\": height\n",
    "                })\n",
    "                \n",
    "                df = pd.read_csv(csv_path, header=None, names=['label', 'x', 'y', 'height', 'width', 'source'])\n",
    "                for _, row in df.iterrows():\n",
    "                    label = clean_and_normalize_label(row['label'])\n",
    "                    if label not in category_id_map:\n",
    "                        category_id = len(category_id_map)\n",
    "                        category_id_map[label] = category_id\n",
    "                        coco_format[\"categories\"].append({\n",
    "                            \"id\": category_id,\n",
    "                            \"name\": label\n",
    "                        })\n",
    "                    \n",
    "                    coco_format[\"annotations\"].append({\n",
    "                        \"id\": annotation_id,\n",
    "                        \"image_id\": image_id,\n",
    "                        \"category_id\": category_id_map[label],\n",
    "                        \"bbox\": [row['x'], row['y'], row['width'], row['height']],\n",
    "                        \"area\": row['width'] * row['height'],\n",
    "                        \"iscrowd\": 0\n",
    "                    })\n",
    "                    annotation_id += 1\n",
    "                \n",
    "                image_id += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {img_file}: {str(e)}\")\n",
    "    \n",
    "    return coco_format, category_id_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "782a5cf3-595a-4349-aab9-f2e74e85f5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_improved_class_hierarchy(category_id_map):\n",
    "    hierarchy = defaultdict(list)\n",
    "    qid_map = {}\n",
    "    \n",
    "    for label, category_id in tqdm(category_id_map.items(), desc=\"Creating class hierarchy\"):\n",
    "        qid = get_wikidata_qid(label)\n",
    "        if qid:\n",
    "            qid_map[label] = qid\n",
    "            wiki_hierarchy = get_wikidata_hierarchy(qid)\n",
    "            for child, parent, _, _ in wiki_hierarchy:\n",
    "                hierarchy[parent].append(child)\n",
    "    \n",
    "    # Improve hierarchy order\n",
    "    G = nx.DiGraph(hierarchy)\n",
    "    sorted_hierarchy = nx.topological_sort(G)\n",
    "    return {node: hierarchy[node] for node in sorted_hierarchy}, qid_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88f9f221-bfa9-4e2e-ab5f-37ebe5766e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikidata_qid(label):\n",
    "    url = \"https://www.wikidata.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"wbsearchentities\",\n",
    "        \"format\": \"json\",\n",
    "        \"language\": \"en\",\n",
    "        \"search\": label\n",
    "    }\n",
    "    headers = {\n",
    "        'User-Agent': 'FactoryNetHackathon/1.0 (https://github.com/yourusername/factorynet-hackathon; youremail@example.com)'\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, params=params, headers=headers)\n",
    "    data = response.json()\n",
    "    \n",
    "    if data.get('search'):\n",
    "        return data['search'][0]['id']\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0858003a-9e2f-4df1-8722-469b058b77fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikidata_hierarchy(qid, max_depth=5):\n",
    "    query = f\"\"\"\n",
    "    SELECT ?item ?itemLabel ?parent ?parentLabel\n",
    "    WHERE {{\n",
    "      wd:{qid} wdt:P279* ?item.\n",
    "      OPTIONAL {{ ?item wdt:P279 ?parent. }}\n",
    "      SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }}\n",
    "    }}\n",
    "    LIMIT {max_depth}\n",
    "    \"\"\"\n",
    "    \n",
    "    url = \"https://query.wikidata.org/sparql\"\n",
    "    headers = {\n",
    "        'User-Agent': 'FactoryNetHackathon/1.0 (https://github.com/yourusername/factorynet-hackathon; youremail@example.com)'\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, params={'query': query, 'format': 'json'}, headers=headers)\n",
    "    data = response.json()\n",
    "    \n",
    "    hierarchy = []\n",
    "    for item in data['results']['bindings']:\n",
    "        child_qid = item['item']['value'].split('/')[-1]\n",
    "        child_label = item['itemLabel']['value']\n",
    "        parent_qid = item.get('parent', {}).get('value', '').split('/')[-1]\n",
    "        parent_label = item.get('parentLabel', {}).get('value')\n",
    "        if parent_qid and parent_label:\n",
    "            hierarchy.append((child_qid, parent_qid, child_label, parent_label))\n",
    "    \n",
    "    return hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "459a5034-a930-4126-a930-c422f1c4265b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNeXtClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, pretrained=True):\n",
    "        super(ConvNeXtClassifier, self).__init__()\n",
    "        self.model = create_model('convnext_tiny', pretrained=pretrained, num_classes=num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bc5f3e1-eed3-4f9d-a716-1093616e0420",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_weights(dataset):\n",
    "    class_counts = defaultdict(int)\n",
    "    for instance in dataset.instances:\n",
    "        class_counts[instance['category_id']] += 1\n",
    "    \n",
    "    total_samples = len(dataset)\n",
    "    class_weights = {class_id: total_samples / count for class_id, count in class_counts.items()}\n",
    "    return class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e9810d3-f9a7-4237-8d53-17f4d4695f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weighted_sampler(dataset):\n",
    "    class_counts = defaultdict(int)\n",
    "    for instance in dataset.instances:\n",
    "        class_counts[instance['category_id']] += 1\n",
    "    \n",
    "    total_samples = len(dataset)\n",
    "    class_weights = {class_id: total_samples / count for class_id, count in class_counts.items()}\n",
    "    \n",
    "    # Use the actual dataset indices\n",
    "    sample_weights = [class_weights[dataset.instances[i]['category_id']] for i in range(len(dataset))]\n",
    "    return WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e938117-45d7-4418-9533-41cabc488384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=100, patience=20):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_wts = model.state_dict()\n",
    "    history = {\n",
    "        'train_loss': [], 'val_loss': [],\n",
    "        'train_accuracy': [], 'val_accuracy': [],\n",
    "        'train_f1': [], 'val_f1': []\n",
    "    }\n",
    "    no_improve = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "                dataloader = train_loader\n",
    "            else:\n",
    "                model.eval()\n",
    "                dataloader = val_loader\n",
    "\n",
    "            running_loss = 0.0\n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "\n",
    "            for inputs, labels in tqdm(dataloader, desc=phase):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloader.dataset)\n",
    "            epoch_accuracy = accuracy_score(all_labels, all_preds)\n",
    "            epoch_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "            \n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Accuracy: {epoch_accuracy:.4f} F1: {epoch_f1:.4f}')\n",
    "\n",
    "            # Record history\n",
    "            history[f'{phase}_loss'].append(epoch_loss)\n",
    "            history[f'{phase}_accuracy'].append(epoch_accuracy)\n",
    "            history[f'{phase}_f1'].append(epoch_f1)\n",
    "\n",
    "            if phase == 'val':\n",
    "                scheduler.step()\n",
    "                if epoch_loss < best_val_loss:\n",
    "                    best_val_loss = epoch_loss\n",
    "                    best_model_wts = model.state_dict()\n",
    "                    no_improve = 0\n",
    "                else:\n",
    "                    no_improve += 1\n",
    "                \n",
    "                if no_improve >= patience:\n",
    "                    print(f'Early stopping triggered after {epoch+1} epochs')\n",
    "                    model.load_state_dict(best_model_wts)\n",
    "                    return model, history\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a9e1476-499d-43be-a943-006092c883b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(history['train_loss'], label='Train')\n",
    "    plt.plot(history['val_loss'], label='Validation')\n",
    "    plt.title(\"Loss Over Time\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(history['train_accuracy'], label='Train')\n",
    "    plt.plot(history['val_accuracy'], label='Validation')\n",
    "    plt.title(\"Accuracy Over Time\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(history['train_f1'], label='Train')\n",
    "    plt.plot(history['val_f1'], label='Validation')\n",
    "    plt.title(\"F1 Score Over Time\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"F1 Score\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"training_history.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c1117e2-23f2-4572-8fa2-881724d4b9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating COCO dataset: 100%|██████████| 6358/6358 [00:36<00:00, 173.34it/s]\n",
      "Creating class hierarchy: 100%|██████████| 4077/4077 [25:07<00:00,  2.70it/s] \n"
     ]
    }
   ],
   "source": [
    "data_dir = '/scratch/rpushpar/ASME_Hackathon/hackathon/data'\n",
    "coco_json_path = 'factorynet_coco_bbox.json'\n",
    "\n",
    "excluded_files = [\n",
    "    '1711599094686.jpg', '1711596276749.jpg', '1711585942345.jpg',\n",
    "    '1711568830061.jpg', '1711569373889.jpg', '1711597390150.jpg',\n",
    "    '1711569098811.jpg', '1711586005189.jpg', '1711577373063.jpg',\n",
    "    '1711568023828.jpg'\n",
    "]\n",
    "\n",
    "# Create COCO format dataset if it doesn't exist\n",
    "if not os.path.exists(coco_json_path):\n",
    "    coco_data, category_id_map = create_coco_dataset(data_dir, excluded_files)\n",
    "    with open(coco_json_path, 'w') as f:\n",
    "        json.dump(coco_data, f)\n",
    "else:\n",
    "    with open(coco_json_path, 'r') as f:\n",
    "        coco_data = json.load(f)\n",
    "    category_id_map = {cat['name']: cat['id'] for cat in coco_data['categories']}\n",
    "\n",
    "# Create class hierarchy\n",
    "hierarchy, qid_map = create_improved_class_hierarchy(category_id_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b727f50-fb3a-49c0-9195-23f3368f6590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique classes: 4077\n",
      "Number of hierarchical relationships: 4998\n"
     ]
    }
   ],
   "source": [
    "# Save output files\n",
    "with open('classes.txt', 'w') as f:\n",
    "    for qid in qid_map.values():\n",
    "        f.write(f\"{qid}\\n\")\n",
    "\n",
    "with open('entities.txt', 'w') as f:\n",
    "    for label, qid in qid_map.items():\n",
    "        f.write(f\"{qid}\\trdfs:label\\t{label}\\n\")\n",
    "    for parent, children in hierarchy.items():\n",
    "        for child in children:\n",
    "            f.write(f\"{child}\\tsubclassOf\\t{parent}\\n\")\n",
    "\n",
    "# Print hierarchy statistics\n",
    "print(f\"Number of unique classes: {len(category_id_map)}\")\n",
    "print(f\"Number of hierarchical relationships: {sum(len(children) for children in hierarchy.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fea4934f-c1af-46e5-8d3c-3c1b0e4207af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets with improved data augmentation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
    "    autoaugment.RandAugment(num_ops=2, magnitude=9),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "full_dataset = FactoryNetBBoxDataset(coco_json_path, data_dir, transform=train_transform)\n",
    "\n",
    "# Create weighted sampler for the full dataset\n",
    "full_sampler = create_weighted_sampler(full_dataset)\n",
    "\n",
    "# Split the dataset\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# Create subset samplers\n",
    "train_sampler = torch.utils.data.SubsetRandomSampler(train_dataset.indices)\n",
    "val_sampler = torch.utils.data.SubsetRandomSampler(val_dataset.indices)\n",
    "\n",
    "# Apply different transforms\n",
    "train_dataset.dataset.transform = train_transform\n",
    "val_dataset.dataset.transform = val_transform\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(full_dataset, batch_size=32, sampler=train_sampler, num_workers=4)\n",
    "val_loader = DataLoader(full_dataset, batch_size=32, sampler=val_sampler, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f6a54a3-c8ac-4a85-b5d1-0d5693f80393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 328/328 [05:02<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 6.1865 Accuracy: 0.0152 F1: 0.0024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 82/82 [01:13<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.5225 Accuracy: 0.0252 F1: 0.0043\n",
      "Epoch 2/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 328/328 [04:45<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 5.6842 Accuracy: 0.0389 F1: 0.0134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 82/82 [01:09<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.4853 Accuracy: 0.0455 F1: 0.0179\n",
      "Epoch 3/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 328/328 [04:41<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 4.9182 Accuracy: 0.0871 F1: 0.0498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 82/82 [01:10<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.4299 Accuracy: 0.0630 F1: 0.0362\n",
      "Epoch 4/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 328/328 [04:40<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 3.7514 Accuracy: 0.2401 F1: 0.1963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 82/82 [01:10<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.4335 Accuracy: 0.0673 F1: 0.0460\n",
      "Epoch 5/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 328/328 [04:37<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 2.7183 Accuracy: 0.4699 F1: 0.4496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 82/82 [01:15<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.4604 Accuracy: 0.0722 F1: 0.0586\n",
      "Epoch 6/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 328/328 [04:44<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 2.1519 Accuracy: 0.5692 F1: 0.5564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 82/82 [01:08<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.4639 Accuracy: 0.0734 F1: 0.0649\n",
      "Epoch 7/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 328/328 [04:43<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.8574 Accuracy: 0.6031 F1: 0.5911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 82/82 [01:10<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.4712 Accuracy: 0.0757 F1: 0.0680\n",
      "Epoch 8/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 328/328 [04:41<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.6737 Accuracy: 0.6235 F1: 0.6113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 82/82 [01:13<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.4857 Accuracy: 0.0780 F1: 0.0725\n",
      "Epoch 9/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 328/328 [04:39<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.5505 Accuracy: 0.6478 F1: 0.6316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 82/82 [01:08<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.4863 Accuracy: 0.0780 F1: 0.0721\n",
      "Epoch 10/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 328/328 [04:40<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.4783 Accuracy: 0.6848 F1: 0.6626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 82/82 [01:14<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.4898 Accuracy: 0.0787 F1: 0.0730\n",
      "Epoch 11/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 328/328 [05:33<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 2.2658 Accuracy: 0.5664 F1: 0.5532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 82/82 [01:29<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.5097 Accuracy: 0.0711 F1: 0.0599\n",
      "Epoch 12/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 328/328 [06:04<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 2.1924 Accuracy: 0.5870 F1: 0.5739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 82/82 [01:35<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.5123 Accuracy: 0.0676 F1: 0.0600\n",
      "Epoch 13/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 328/328 [05:57<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 2.0138 Accuracy: 0.6091 F1: 0.5967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 82/82 [01:20<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.4965 Accuracy: 0.0764 F1: 0.0688\n",
      "Epoch 14/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 328/328 [05:05<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.8997 Accuracy: 0.6160 F1: 0.6042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 82/82 [01:15<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.4860 Accuracy: 0.0711 F1: 0.0636\n",
      "Epoch 15/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 328/328 [04:39<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.8022 Accuracy: 0.6299 F1: 0.6196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 82/82 [01:12<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.4942 Accuracy: 0.0726 F1: 0.0662\n",
      "Epoch 16/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 328/328 [04:44<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.7290 Accuracy: 0.6346 F1: 0.6230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 82/82 [01:11<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.4819 Accuracy: 0.0791 F1: 0.0727\n",
      "Epoch 17/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 328/328 [04:51<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.6690 Accuracy: 0.6369 F1: 0.6257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 82/82 [01:14<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.4851 Accuracy: 0.0730 F1: 0.0654\n",
      "Epoch 18/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 328/328 [04:42<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.6198 Accuracy: 0.6413 F1: 0.6304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 82/82 [01:10<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.4836 Accuracy: 0.0772 F1: 0.0686\n",
      "Epoch 19/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 328/328 [04:44<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.5936 Accuracy: 0.6472 F1: 0.6346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 82/82 [01:13<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.4869 Accuracy: 0.0734 F1: 0.0685\n",
      "Epoch 20/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 328/328 [04:56<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.5385 Accuracy: 0.6473 F1: 0.6377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 82/82 [01:14<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.4891 Accuracy: 0.0783 F1: 0.0719\n",
      "Epoch 21/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 328/328 [04:54<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.4981 Accuracy: 0.6521 F1: 0.6415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 82/82 [01:15<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.5026 Accuracy: 0.0776 F1: 0.0710\n",
      "Epoch 22/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 328/328 [04:55<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.4628 Accuracy: 0.6523 F1: 0.6405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 82/82 [01:15<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.4922 Accuracy: 0.0760 F1: 0.0681\n",
      "Epoch 23/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 328/328 [04:49<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.4291 Accuracy: 0.6541 F1: 0.6416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 82/82 [01:13<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.5103 Accuracy: 0.0741 F1: 0.0683\n",
      "Early stopping triggered after 23 epochs\n",
      "Training completed. Model saved as 'convnext_factorynet_bbox_model.pth'.\n",
      "Class hierarchy information saved in 'classes.txt' and 'entities.txt'.\n"
     ]
    }
   ],
   "source": [
    "# Initialize model with pre-trained weights\n",
    "model = ConvNeXtClassifier(num_classes=len(category_id_map), pretrained=True)\n",
    "\n",
    "# Training setup with improved hyperparameters\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0005, weight_decay=0.05)\n",
    "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-6)\n",
    "\n",
    "# Train the model\n",
    "model, history = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=50, patience=20)\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'convnext_factorynet_bbox_model.pth')\n",
    "\n",
    "print(\"Training completed. Model saved as 'convnext_factorynet_bbox_model.pth'.\")\n",
    "print(\"Class hierarchy information saved in 'classes.txt' and 'entities.txt'.\")\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c3c9f2-ebbe-438f-8d8f-2fe9f540b4c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv.ASMEHackathon",
   "language": "python",
   "name": "venv.asmehackathon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
